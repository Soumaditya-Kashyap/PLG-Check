# ğŸ“„ Plagiarism Checker - Data Scraping System

## ğŸ¯ **What This Does**
Processes uploaded PDFs, extracts relevant research content from ArXiv and web sources, and stores them in organized JSON format for analysis.

## ğŸš€ **Current Features**
- âœ… PDF text extraction and chunking
- âœ… AI keyword generation (Google Gemini)
- âœ… ArXiv research paper downloading
- âœ… Web content scraping (Tavily API)
- âœ… Structured JSON data storage

## ğŸ“ **Backend File Structure**
```
backend/
â”œâ”€â”€ app.py                          # Main Flask server & API endpoints
â”œâ”€â”€ config.py                       # Configuration settings & constants
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ data_collection_service.py  # Main orchestrator - coordinates all services
â”‚   â”œâ”€â”€ llm_service.py             # Google Gemini AI integration
â”‚   â”œâ”€â”€ arxiv_service.py           # ArXiv paper search & download
â”‚   â””â”€â”€ tavily_service.py          # Web content scraping & extraction
â””â”€â”€ utils/
    â””â”€â”€ smart_text_processor.py    # PDF processing & text chunking
```

## ğŸ› ï¸ **Setup**

### **1. Install Dependencies**
```bash
# Windows
cd backend
pip install -r requirements.txt

# Linux/Mac
cd backend
pip3 install -r requirements.txt
```

### **2. API Keys Setup**
Create `backend/.env` file:
```env
GOOGLE_API_KEY=your_google_gemini_api_key
TAVILY_API_KEY=your_tavily_api_key
```

### **3. Run Backend Server**
```bash
# Windows
python app.py

# Linux/Mac
python3 app.py
```
Backend runs at: `http://127.0.0.1:5000`

### **4. Run Frontend (Optional)**
```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```
Frontend runs at: `http://localhost:5173`

## ğŸ“Š **API Usage**

### **Upload PDF**
```bash
curl -X POST -F "file=@document.pdf" http://127.0.0.1:5000/upload
```

### **Response**
```json
{
  "success": true,
  "message": "File processing completed successfully",
  "results": {
    "chunks_created": 45,
    "keywords_extracted": 15,
    "arxiv_papers_found": 8,
    "web_content_scraped": 12
  }
}
```

## ğŸ“ **Generated Data Structure**
```
results/DOCUMENT_ID/
â”œâ”€â”€ chunks.json     # PDF text chunks
â””â”€â”€ keywords.json   # AI-generated keywords

scraped_data/DOCUMENT_ID/
â”œâ”€â”€ arxiv/          # Downloaded research papers
â”‚   â”œâ”€â”€ arxiv_results.json
â”‚   â””â”€â”€ pdfs/
â””â”€â”€ web/            # Scraped web content  
    â”œâ”€â”€ web_results.json
    â””â”€â”€ content/
```

## ğŸ”§ **Required API Keys**
- **Google Gemini AI** - For keyword generation
- **Tavily API** - For web content scraping

## âœ… **System Status**
**Phase 1 Complete**: Data Collection & Storage  
**Ready For**: Vector Embeddings & Similarity Analysis

## ğŸ”„ **How It Works (Workflow)**

### **Step 1: PDF Upload**
User uploads academic PDF â†’ `app.py` receives file â†’ saves to `uploads/` folder

### **Step 2: Text Processing**
`smart_text_processor.py` â†’ extracts text â†’ creates chunks â†’ saves to `results/DOCUMENT_ID/chunks.json`

### **Step 3: AI Keyword Generation**
`llm_service.py` â†’ sends text to Google Gemini â†’ generates search keywords â†’ saves to `results/DOCUMENT_ID/keywords.json`

### **Step 4: Research Paper Collection**
`arxiv_service.py` â†’ searches ArXiv using keywords â†’ downloads relevant PDFs â†’ saves to `scraped_data/DOCUMENT_ID/arxiv/`

### **Step 5: Web Content Scraping**
`tavily_service.py` â†’ searches web using keywords â†’ extracts clean content â†’ saves to `scraped_data/DOCUMENT_ID/web/`

### **Step 6: Data Ready**
All content organized in JSON format â†’ ready for similarity analysis â†’ plagiarism detection can begin

**ğŸ¯ Result: Complete academic content database for uploaded PDF**

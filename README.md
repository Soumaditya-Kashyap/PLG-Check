# ğŸ“„ Plagiarism Checker - Data Scraping & Embedding System

## ğŸ¯ **What This Does**
Processes uploaded PDFs, extracts relevant research content from ArXiv and web sources, creates vector embeddings for similarity analysis, and prepares everything for plagiarism detection.

## ğŸš€ **Current Features**
- âœ… PDF text extraction and smart chunking
- âœ… AI keyword generation (Google Gemini)
- âœ… ArXiv research paper downloading
- âœ… Web content scraping (Tavily API)
- âœ… **Vector embedding creation (FAISS)**
- âœ… **Organized similarity database**
- âœ… **Ready for plagiarism detection**

## ğŸ“ **Backend File Structure**
```
backend/
â”œâ”€â”€ app.py                          # Main Flask server & API endpoints
â”œâ”€â”€ config.py                       # Configuration settings & constants
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ data_collection_service.py  # Main orchestrator - coordinates all services
â”‚   â”œâ”€â”€ llm_service.py             # Google Gemini AI integration
â”‚   â”œâ”€â”€ arxiv_service.py           # ArXiv paper search & download
â”‚   â”œâ”€â”€ tavily_service.py          # Web content scraping & extraction
â”‚   â””â”€â”€ faiss_vector_service.py    # FAISS vector database & embeddings
â””â”€â”€ utils/
    â””â”€â”€ smart_text_processor.py    # PDF processing & text chunking
```

## ğŸ› ï¸ **Setup**

### **1. Install Dependencies**
```bash
# Windows
cd backend
pip install -r requirements.txt

# Linux/Mac
cd backend
pip3 install -r requirements.txt
```

### **2. API Keys Setup**
Create `backend/.env` file:
```env
GOOGLE_API_KEY=your_google_gemini_api_key
TAVILY_API_KEY=your_tavily_api_key
```

### **3. Run Backend Server**
```bash
# Windows
python app.py

# Linux/Mac
python3 app.py
```
Backend runs at: `http://127.0.0.1:5000`

### **4. Run Frontend (Optional)**
```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```
Frontend runs at: `http://localhost:5173`

## ğŸ“Š **API Usage**

### **Upload PDF**
```bash
curl -X POST -F "file=@document.pdf" http://127.0.0.1:5000/upload
```

### **Response**
```json
{
  "success": true,
  "message": "File processing completed successfully",
  "results": {
    "chunks_created": 45,
    "keywords_extracted": 15,
    "arxiv_papers_found": 8,
    "web_content_scraped": 12,
    "embeddings_created": 65,
    "faiss_vectors_total": 1247
  }
}
```

## ğŸ“ **Generated Data Structure**
```
results/DOCUMENT_ID/
â”œâ”€â”€ chunks.json     # PDF text chunks
â””â”€â”€ keywords.json   # AI-generated keywords

scraped_data/DOCUMENT_ID/
â”œâ”€â”€ arxiv/          # Downloaded research papers
â”‚   â”œâ”€â”€ arxiv_results.json
â”‚   â””â”€â”€ pdfs/
â””â”€â”€ web/            # Scraped web content  
    â”œâ”€â”€ web_results.json
    â””â”€â”€ content/

faiss_vector_db/
â”œâ”€â”€ user_pdf_index.bin         # User PDF embeddings
â”œâ”€â”€ user_pdf_metadata.pkl      # User PDF metadata
â”œâ”€â”€ arxiv_papers_index.bin     # ArXiv paper embeddings
â”œâ”€â”€ arxiv_papers_metadata.pkl  # ArXiv metadata
â”œâ”€â”€ web_content_index.bin      # Web content embeddings
â”œâ”€â”€ web_content_metadata.pkl   # Web metadata
â””â”€â”€ document_metadata.pkl      # Document tracking
```

## ğŸ”§ **Required API Keys**
- **Google Gemini AI** - For keyword generation
- **Tavily API** - For web content scraping

## âœ… **System Status**
**Phase 1 Complete**: Data Collection, Storage & Embedding Creation  
**Ready For**: Advanced Similarity Analysis & Plagiarism Detection

## ğŸ”„ **How It Works (Workflow)**

### **Step 1: PDF Upload**
User uploads academic PDF â†’ `app.py` receives file â†’ saves to `uploads/` folder

### **Step 2: Text Processing**
`smart_text_processor.py` â†’ extracts text â†’ creates chunks â†’ saves to `results/DOCUMENT_ID/chunks.json`

### **Step 3: AI Keyword Generation**
`llm_service.py` â†’ sends text to Google Gemini â†’ generates search keywords â†’ saves to `results/DOCUMENT_ID/keywords.json`

### **Step 4: Research Paper Collection**
`arxiv_service.py` â†’ searches ArXiv using keywords â†’ downloads relevant PDFs â†’ saves to `scraped_data/DOCUMENT_ID/arxiv/`

### **Step 5: Web Content Scraping**
`tavily_service.py` â†’ searches web using keywords â†’ extracts clean content â†’ saves to `scraped_data/DOCUMENT_ID/web/`

### **Step 6: Vector Embedding Creation**
`faiss_vector_service.py` â†’ creates embeddings for all content:
- **User PDF**: Text chunks â†’ 384-dim vectors â†’ `user_pdf_index.bin`
- **ArXiv Papers**: Paper content â†’ vectors â†’ `arxiv_papers_index.bin`  
- **Web Content**: Article text â†’ vectors â†’ `web_content_index.bin`

### **Step 7: Organized Vector Storage**
All embeddings stored in separate FAISS indexes â†’ metadata tracking â†’ ready for semantic similarity search

**ğŸ¯ Result: Complete vector database with organized embeddings for plagiarism detection**
